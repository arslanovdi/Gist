project:
  debug: true
  shutdownTimeout: 60s
  ttl: 1h

bot:
  token: ""
  ngrok_auth_token: ""
  ngrok_domain: ""
  read_header_timeout: 30m

client:
  app_id: 0
  app_hash: ""
  user_id: 0
  phone: ""
  sessionTTL: 30m
  requestTimeout: 5m

settings:
  chat_unread_threshold: 1

llm:
  development: true
  flow_timeout: 30m # тайм-аут выполнения сценария LLM
  drift_percent: 10 # процент отклонения от заданного контекстного окна (в минус), так как количество токенов можно посчитать только приблизительно.
  symbol_per_token: 2 # 1 токен ~ 2-3 символа. Расчет приблизительный, так как неизвестно как работают токенизаторы различных LLM.
  messages_per_batch: 800 # максимальное количество сообщений в одном запросе к LLM, меньше может быть если не хватает контекстного окна или столько просто нет))

  client_type: "OpenRouter" #  Ollama, OpenRouter, Gemini, OpenAI
  Ollama:
    server_address: "http://host.docker.internal:11434"
    timeout: 60     #seconds
    model: "glm-4.6:cloud"
    context_window: 200_000  # контекстное окно LLM
  OpenRouter:
    model: "xiaomi/mimo-v2-flash:free" #"google/gemini-2.0-flash-exp:free" #"xiaomi/mimo-v2-flash:free" #"tngtech/deepseek-r1t2-chimera:free"
    context_window: 260_000 #262_144# контекстное окно LLM
  Gemini:
    model: "gemini-2.5-flash"
    context_window: 1_000_000  # контекстное окно LLM
  OpenAI:
    model: "gpt-4o"
    context_window: 128_000  # контекстное окно LLM

llm_tts:
  development: true
  flow_timeout: 30m # тайм-аут выполнения сценария LLM

  client_type: "Gemini" #Gemini
  Gemini:
    model: "gemini-2.5-flash-preview-tts"
    language_code: "ru-RU"
    voice_name: "Leda"
